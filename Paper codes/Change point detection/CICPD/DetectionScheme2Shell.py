#!/usr/local/bin/python
# -*-coding:utf-8 -*-

'''
This code is used for get the embedding result of the sequential networks.
First,we construct a graph with which each network in the sequence is modeled
as a node. We compute kl divergence  between each two sequential network
and view inverse of those kl divergence as the weight of corresponding
edge involves corresponding networks.

@Author: TingtingZhu
@email:  tingtingzhu93@gmail.com
@Date: 2017-11
@Last Modified by: TingtingZhu 2020
'''
import os
import sys
import warnings
from collections import OrderedDict
from functools import partial
from operator import itemgetter
from optparse import OptionParser

import networkx as nx
import scipy.io as scio
import scipy.stats
import numpy as np
from  numpy import sort
from scipy import sparse
from scipy.sparse.csgraph import connected_components
from scipy.spatial import distance
from scipy.spatial.distance import correlation, euclidean, cosine
from sklearn import metrics
from sklearn.cluster import SpectralClustering
from sklearn.manifold import spectral_embedding
from sklearn.utils import check_symmetric

from evaluation import Evaluation

flag_dict = {"0": False, "1": True}

toynetwork0406_Ground = set([1, 3, 4, 6, 7, 9, 10, 12])
MIT_Ground_Week = set([1, 7, 8, 10, 11, 12, 14, 15, 16, 18, 20, 22,
                       26, 27, 29, 30, 36, 37, 38, 39])

Enron_Ground_month = set(
    [12, 14, 20, 21, 23, 24, 25, 25, 26, 28, 29, 30, 32, 32, 33, 36,
     36, 36, 36, 37, 38, 39, 39, 39, 40])
Cosponsor_Ground = set([3, 7])
groundTruth_dict = {"0": toynetwork0406_Ground, "1": MIT_Ground_Week,
                    "2": Enron_Ground_month, "4": Cosponsor_Ground}

prScore = []


def kernel_js(p, q):
    '''
    :param p:
    :param q:
    :return:
    '''
    M = (p + q) / 2
    # defaults to ``e``
    js2 = 0.5 * scipy.stats.entropy(p, M, base=np.e) + \
          0.5 * scipy.stats.entropy(q, M, base=np.e)
    return js2


kernel_mode_dict = {"js": kernel_js}


def tmp(alpha, ds):
    return np.exp(-(pow(ds, 2)) / (2.0 * alpha * alpha))


def largestConnectedComponent(motifAdjacencyMat, graph_weighted):
    '''
    largestConnectedComponent gets the largest connected Component of the graph
    params:
        motifAdjacencyMat :
            the weighted adjacencyMatrix where the(i,j) entry counts
            # i,j appears in specified motif
        graph_weighted:
            the graph generated by motifAdjacencyMat
    returns:
        LCC: a part of the motifAdjacencyMat(extracted for the largestConnectedComponent)
        Lcc_index: a list whose item is the node in the largestConnectedComponent
    '''
    con = nx.connected_components(graph_weighted)
    con_components = list(con)
    Lcc_index = sort(list(con_components[0]))
    LCC = motifAdjacencyMat[Lcc_index, :][:, Lcc_index]
    return LCC, Lcc_index


def _graph_connected_component(graph, node_id):
    """Find the largest graph connected components that contains one
    given node

    Parameters
    ----------
    graph : array-like, shape: (n_samples, n_samples)
        adjacency matrix of the graph, non-zero weight means an edge
        between the nodes

    node_id : int
        The index of the query node of the graph

    Returns
    -------
    connected_components_matrix : array-like, shape: (n_samples,)
        An array of bool value indicating the indexes of the nodes
        belonging to the largest connected components of the given query
        node
    """
    n_node = graph.shape[0]
    if sparse.issparse(graph):
        # speed up row-wise access to boolean connection mask
        graph = graph.tocsr()
    connected_nodes = np.zeros(n_node, dtype=np.bool)
    nodes_to_explore = np.zeros(n_node, dtype=np.bool)
    nodes_to_explore[node_id] = True
    for _ in range(n_node):
        last_num_component = connected_nodes.sum()
        np.logical_or(connected_nodes, nodes_to_explore, out=connected_nodes)
        if last_num_component >= connected_nodes.sum():
            break
        indices = np.where(nodes_to_explore)[0]
        nodes_to_explore.fill(False)
        for i in indices:
            if sparse.issparse(graph):
                neighbors = graph[i].toarray().ravel()
            else:
                neighbors = graph[i]
            np.logical_or(nodes_to_explore, neighbors, out=nodes_to_explore)
    return connected_nodes


def _graph_is_connected(graph):
    """ Return whether the graph is connected (True) or Not (False)

    Parameters
    ----------
    graph : array-like or sparse matrix, shape: (n_samples, n_samples)
        adjacency matrix of the graph, non-zero weight means an edge
        between the nodes

    Returns
    -------
    is_connected : bool
        True means the graph is fully connected and False means not
    """
    if sparse.isspmatrix(graph):
        # sparse graph, find all the connected components
        n_connected_components, _ = connected_components(graph)
        return n_connected_components == 1
    else:
        # dense graph, find all connected components start from node 0
        return _graph_connected_component(graph, 0).sum() == graph.shape[0]


def constructNetwork(tempM, flag):
    """construct network using the given matrix if flag is true
    else using the complemented result of the given matrix

    Parameters
    ----------
    tempM:the given matrix
    flag: True or False

    Returns
    ----------
    graph:the constructed graph  the directed network based on the original network
    if flag is True else based on the complemented network
    """
    if flag:
        graph = nx.from_numpy_matrix(tempM)
        graph = graph.to_directed()
        return graph

    tempM[tempM != 0] = 1
    m, n = np.shape(tempM)
    newtempM = np.ones((m, n)) - tempM
    newtempM = newtempM - np.diag(np.diag(np.ones((m, n))))

    graph = nx.from_numpy_matrix(newtempM)
    graph = graph.to_directed()  #
    return graph


def getPrValue(tempGraph, alpha1):
    """get the pr value of each network
    Parameters
    ----------
    tempGraph:
    alpha1:skip probability

    Returns
    ----------
    prvalue:

    """
    return nx.pagerank(tempGraph, alpha=alpha1)


def dealsome(pr, flag):
    """get pr value if flag is true else get normalized rank
    Parameters
    ----------
    pr:
    flag:

    Returns
    ----------

    """
    if flag:
        if int(sum(pr.values())) - 1 == 0:
            node2score = [pr[i] for i in range(len(pr))]
            return np.array(node2score)
        else:
            total = sum(pr.values()) * 1.0
            node2score = [pr[i] / total for i in range(len(pr))]
            return np.array(node2score)

    sortedResult = OrderedDict(sorted(pr.iteritems(), key=itemgetter(1), reverse=True))
    top = [0] * len(pr)
    for i in range(1, len(pr) + 1):
        nodeId, score = sortedResult.popitem()
        top[nodeId] = i
    top = [np.exp(value) for value in top]
    total = sum(top) * 1.0
    top = [value / total for value in top]
    return np.array(top)


def get_prvalue_sequence(path, alpha, flag1, flag2):
    """get the pr-score sequence
    Parameters
    ----------
    path:
    alpha:
    flag1:
    flag2:

    Returns
    ----------

    """
    matrixWhole = scio.mmread(path).todense()
    numSnapshots, numNodesSquare = np.shape(matrixWhole)
    numNodes = int(np.sqrt(numNodesSquare))

    for i in range(numSnapshots):
        tempmm = np.reshape(matrixWhole[i], (numNodes, numNodes))
        tempmm = tempmm - np.diag(np.diag(tempmm))
        pr = getPrValue(constructNetwork(tempmm, flag1), alpha)
        prScore.append(dealsome(pr, flag2))


def predictResult(assignments):
    """Get the potential changePoints
    This function takes the resultant assignments(type:list) as input,
    such as [1,1,1,3,3,3,2,2,2,1,1,2,3,3,1,3,3,2...]ï¼Œthen using the
    specific spatial information inside the data to find possible changePoint.

    Parameters
    ----------
    assignments: the label of each point which
    given by clustering method

    Returns
    ----------
    p_c: potential changePoints
    """
    print "assignments is ", assignments
    predicts = []
    temp = assignments[0]
    for i in range(1, len(assignments)):
        if temp == assignments[i]:
            continue
        predicts.append(i)
        temp = assignments[i]

    p_c = set(predicts)
    return p_c


def detect_change_points(path, groundEvent, nodeImportance, alpha_pg, flag1, flag2,
                         s, kernel_fun):
    """Use the spectral clustering implemented in sklearn (Mode: fully connected )
    (only two parameters to tune : alpha, k)
    Parameters
    ----------
        path: the path of the sequence networks
        groundEvent:ground event information
        nodeImportance:
        alpha_pg: skip probability for pageRank
        flag1: for construct the network(True : original; False : supplement )
        flag2: for construct the probability distribution(True: pageRank
                or leaderRank; False: normalized Rank)
        s:the delay delta
        kernel_fun: used for adjacency/affinity matrix computation.

    Returns
    ----------

    """
    print "step 1"
    get_prvalue_sequence(path, alpha_pg, flag1, flag2)

    print "Step 2"
    numSnapshots = len(prScore)
    numNodes = len(prScore[0])
    print "there are totally %d snapshots,each snapshot has %d nodes" \
          % (numSnapshots, numNodes)

    temp_alpha = 0.0
    temp_k = 0
    temp_f = 0.0

    distance_array = distance.pdist(np.reshape(np.array(prScore), (numSnapshots, numNodes)),
                                    kernel_fun)
    for alpha in range(1, 101, 1):
        alpha = alpha / 100.0
        func = partial(tmp, alpha)
        condensed_similarity = map(func, distance_array)
        adjacentM = distance.squareform(condensed_similarity)

        adjacentM = check_symmetric(adjacentM)
        if not _graph_is_connected(adjacentM):
            print "not fully connected alpha: ", alpha
            warnings.warn("Graph is not fully connected.")
            graph_weighted = nx.from_numpy_matrix(adjacentM)
            Lcc, p = largestConnectedComponent(adjacentM, graph_weighted)
            if len(Lcc) < 1.0 * numSnapshots:
                print "connected nodes: ", len(Lcc)
                continue
            else:
                print "connected nodes: ", len(Lcc)

        else:
            graph_weighted = nx.from_numpy_matrix(adjacentM)
            Lcc, p = largestConnectedComponent(adjacentM, graph_weighted)
            print "connected nodes: ", len(Lcc)

        best_k = -1
        best_score = -np.inf
        for index, k in enumerate((2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12)):

            if k >= numSnapshots:
                break
            try:
                score = 0.0
                for i in range(10):
                    model = SpectralClustering(n_clusters=k, assign_labels='discretize',
                                               affinity='precomputed',
                                               random_state=1).fit(adjacentM)
                    assigments = model.labels_
                    smallkEigVector = spectral_embedding(np.array(adjacentM), n_components=k,
                                                         eigen_solver=None,
                                                         random_state=None, eigen_tol=0.0,
                                                         norm_laplacian=True, drop_first=False)

                    score += metrics.silhouette_score(np.reshape(np.array(smallkEigVector),
                                                                 (numSnapshots, k)),
                                                      assigments)

                score = score / 10.0

                if score > best_score:
                    best_score = score
                    best_k = k
            except Exception, e:
                print "except, ", e

        model = SpectralClustering(n_clusters=best_k, assign_labels='discretize',
                                   affinity='precomputed',
                                   random_state=1).fit(adjacentM)
        best_assigments = model.labels_

        try:
            p_c = predictResult(best_assigments)

            precision, recall, fvalue, fpr = Evaluation(p_c, groundEvent, s,
                                                        numSnapshots, K=best_k, alpha=alpha)
            if fvalue > temp_f:
                temp_f = fvalue
                temp_alpha = alpha
                temp_k = best_k
        except:
            print "no changes detected"

    print "the best fvalue is %f, the k is %d and the alpha is %f" % \
          (temp_f, temp_k, temp_alpha)


def check_parameters(**kw):
    '''Check whether the parameters meet the requirements

     Args:
         data:The filename of the data

    '''

    data = kw.get('data', None)
    if data is not None and not os.path.isfile(data):
        print('The data does not exist:{0}.'.format(data))
        return False

    return True


def main(**kw):
    '''
    network_path,ground_truth,nodeImportance,flag1,flag2,s,kernel_mode
    :return:
    '''
    if not check_parameters(**kw):
        return False

    alpha_pg = 0.85
    s = 0
    network_path = kw['data']
    ground_truth = kw['groundtruth']
    nodeImportance = kw['nodeImportance']
    flag1 = kw['flag1']
    flag2 = kw['flag2']
    kernel_mode = kw['kernel_mode']
    flag1 = flag_dict[flag1]
    flag2 = flag_dict[flag2]

    print "data", network_path
    print "nodeImportance", nodeImportance
    print "flag1,flag2", flag1, flag2
    print "kernelFun", kernel_mode
    print "groundTruth", ground_truth

    ground_truth = groundTruth_dict[ground_truth]
    print "ground: ", ground_truth
    print "sliding length: ", s
    kernel_fun = kernel_mode_dict[kernel_mode]

    detect_change_points(network_path, ground_truth, nodeImportance, alpha_pg, flag1, flag2,
                         s, kernel_fun)


if __name__ == "__main__":

    parser = OptionParser(usage="%prog -d data -o out")

    parser.add_option(
        "-d", "--data",
        help=u"The file name of the data(networkData) to be extracted(includes the full path)"
    )

    parser.add_option(
        "-g", "--groundTruth",
        help=u"The groundTruth of the change points,0(Toy),1(MIT),2(Enron)"
    )
    parser.add_option(
        "-n", "--nodeImportance",
        help=u"LeaderRank or PageRank"
    )

    parser.add_option(
        "-z", "--flag2",
        help=u"flag2 1 for True(value) 0 for False(normalized_rank)"
    )
    parser.add_option(
        "-f", "--flag1",
        help=u"flag1 1 for True(original) 0 for False(supplement)"
    )
    parser.add_option(
        "-k", "--kernel_mode",
        help=u"kernel mode: js(js_distance)"
    )

    if not sys.argv[1:]:
        parser.print_help()
        exit(1)

    (opts, args) = parser.parse_args()
    main(data=opts.data, groundtruth=opts.groundTruth, nodeImportance=opts.nodeImportance,
         flag1=opts.flag1,
         flag2=opts.flag2, kernel_mode=opts.kernel_mode)
